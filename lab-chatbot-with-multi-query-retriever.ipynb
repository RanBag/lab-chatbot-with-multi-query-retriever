{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with LangChain, OpenAI, and MultiQuery Retriever\n",
    "\n",
    "This interactive workbook demonstrates example of Elasticsearch's [MultiQuery Retriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) to generate similar queries for a given user input and apply all queries to retrieve a larger set of relevant documents from a vectorstore.\n",
    "\n",
    "Before we begin, we first split the fictional workplace documents into passages with `langchain` and uses OpenAI to transform these passages into embeddings and then store these into Elasticsearch.\n",
    "\n",
    "We will then ask a question, generate similar questions using langchain and OpenAI, retrieve relevant passages from the vector store, and use langchain and OpenAI again to provide a summary for the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\44758\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -qU jq lark langchain langchain-elasticsearch langchain_openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in c:\\users\\44758\\anaconda3\\lib\\site-packages (8.18.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in c:\\users\\44758\\anaconda3\\lib\\site-packages (from elasticsearch) (8.17.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\44758\\anaconda3\\lib\\site-packages (from elasticsearch) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\44758\\anaconda3\\lib\\site-packages (from elasticsearch) (4.13.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\44758\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\44758\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\44758\\anaconda3\\lib\\site-packages (from python-dateutil->elasticsearch) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv langchain langchain-elasticsearch langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy<2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_openai.llms import OpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from getpass import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elasticsearch\n",
    "\n",
    "ℹ️ We're using an Elastic Cloud deployment of Elasticsearch for this notebook. If you don't have an Elastic Cloud deployment, sign up [here](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook) for a free trial. \n",
    "\n",
    "We'll use the **Cloud ID** to identify our deployment, because we are using Elastic Cloud deployment. To find the Cloud ID for your deployment, go to https://cloud.elastic.co/deployments and select your deployment.\n",
    "\n",
    "We will use [ElasticsearchStore](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html) to connect to our elastic cloud deployment, This would help create and index data easily.  We would also send list of documents that we created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
    "\n",
    "# https://platform.openai.com/api-keys\n",
    "OPENAI_API_KEY = getpass(\"OpenAI API key: \")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "vectorstore = ElasticsearchStore(\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    "    index_name= \"qa_workplace\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcessing\n",
    "## Indexing Data into Elasticsearch\n",
    "Let's download the sample dataset and deserialize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen # this just lets me grab stuff from the internet\n",
    "import json\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/example-apps/chatbot-rag-app/data/data.json\"\n",
    "\n",
    "# open the link and read what's insid\n",
    "response = urlopen(url) \n",
    "\n",
    "\n",
    "# turn that downloaded JSON file into actual Python data \n",
    "data = json.load(response)\n",
    "\n",
    "# save that data into a file locally so we don’t have to redownload it every time\n",
    "with open(\"temp.json\", \"w\") as json_file:         \n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Effective: March 2020\\nPurpose\\n\\nThe purpose of this full-time work-from-home policy is to provide guidelines and support for employees to conduct their work remotely, ensuring the continuity and productivity of business operations during the COVID-19 pandemic and beyond.\\nScope\\n\\nThis policy applies to all employees who are eligible for remote work as determined by their role and responsibilities. It is designed to allow employees to work from home full time while maintaining the same level of performance and collaboration as they would in the office.\\nEligibility\\n\\nEmployees who can perform their work duties remotely and have received approval from their direct supervisor and the HR department are eligible for this work-from-home arrangement.\\nEquipment and Resources\\n\\nThe necessary equipment and resources will be provided to employees for remote work, including a company-issued laptop, software licenses, and access to secure communication tools. Employees are responsible for maintaining and protecting the company's equipment and data.\\nWorkspace\\n\\nEmployees working from home are responsible for creating a comfortable and safe workspace that is conducive to productivity. This includes ensuring that their home office is ergonomically designed, well-lit, and free from distractions.\\nCommunication\\n\\nEffective communication is vital for successful remote work. Employees are expected to maintain regular communication with their supervisors, colleagues, and team members through email, phone calls, video conferences, and other approved communication tools.\\nWork Hours and Availability\\n\\nEmployees are expected to maintain their regular work hours and be available during normal business hours, unless otherwise agreed upon with their supervisor. Any changes to work hours or availability must be communicated to the employee's supervisor and the HR department.\\nPerformance Expectations\\n\\nEmployees working from home are expected to maintain the same level of performance and productivity as if they were working in the office. Supervisors and team members will collaborate to establish clear expectations and goals for remote work.\\nTime Tracking and Overtime\\n\\nEmployees are required to accurately track their work hours using the company's time tracking system. Non-exempt employees must obtain approval from their supervisor before working overtime.\\nConfidentiality and Data Security\\n\\nEmployees must adhere to the company's confidentiality and data security policies while working from home. This includes safeguarding sensitive information, securing personal devices and internet connections, and reporting any security breaches to the IT department.\\nHealth and Well-being\\n\\nThe company encourages employees to prioritize their health and well-being while working from home. This includes taking regular breaks, maintaining a work-life balance, and seeking support from supervisors and colleagues when needed.\\nPolicy Review and Updates\\n\\nThis work-from-home policy will be reviewed periodically and updated as necessary, taking into account changes in public health guidance, business needs, and employee feedback.\\nQuestions and Concerns\\n\\nEmployees are encouraged to direct any questions or concerns about this policy to their supervisor or the HR department.\\n\",\n",
      "  \"summary\": \"This policy outlines the guidelines for full-time remote work, including eligibility, equipment and resources, workspace requirements, communication expectations, performance expectations, time tracking and overtime, confidentiality and data security, health and well-being, and policy reviews and updates. Employees are encouraged to direct any questions or concerns\",\n",
      "  \"name\": \"Work From Home Policy\",\n",
      "  \"url\": \"./sharepoint/Work from home policy.txt\",\n",
      "  \"created_on\": \"2020-03-01\",\n",
      "  \"updated_at\": \"2020-03-01\",\n",
      "  \"category\": \"teams\",\n",
      "  \"_run_ml_inference\": true,\n",
      "  \"rolePermissions\": [\n",
      "    \"demo\",\n",
      "    \"manager\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# just peek at the first item to see the structure\n",
    "print(json.dumps(data[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'summary', 'name', 'url', 'created_on', 'updated_at', 'category', '_run_ml_inference', 'rolePermissions'])\n"
     ]
    }
   ],
   "source": [
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for JSON list error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jq in c:\\users\\44758\\anaconda3\\lib\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents into Passages\n",
    "\n",
    "We’ll chunk documents into passages in order to improve the retrieval specificity and to ensure that we can provide multiple passages within the context window of the final question answering prompt.\n",
    "\n",
    "Here we are chunking documents into 800 token passages with an overlap of 400 tokens.\n",
    "\n",
    "Here we are using a simple splitter but Langchain offers more advanced splitters to reduce the chance of context being lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "# helps us break long text into chunks (so the AI doesn't get overwhelmed lol)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "#this function is sto pull some extra info from the data - here so the loader doesn't cry lol\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    # automatically grab all relevant metadata from each record\n",
    "    metadata[\"name\"] = record.get(\"name\")\n",
    "    metadata[\"summary\"] = record.get(\"summary\")\n",
    "    metadata[\"url\"] = record.get(\"url\")\n",
    "    metadata[\"category\"] = record.get(\"category\")\n",
    "    metadata[\"created_on\"] = record.get(\"created_on\")\n",
    "    metadata[\"updated_at\"] = record.get(\"updated_at\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "# For more loaders https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "# And 3rd party loaders https://python.langchain.com/docs/modules/data_connection/document_loaders/#third-party-loaders\n",
    "loader = JSONLoader(\n",
    "    file_path=\"temp.json\",  # our data file from earlier\n",
    "    jq_schema=\".[]\",        # treat it like a list of records\n",
    "    content_key=\"content\",   # this is the actual text we're chunking\n",
    "    metadata_func=metadata_func,  # func we just defined to pull out extra info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text splitter to chunk the docs into small parts\n",
    "# helps with better question answering because AI likes bite-sized pieces\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=800, chunk_overlap=400 \n",
    ")\n",
    "\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 documents from C:\\Users\\44758\\Desktop\\IronHack\\week7\\day4\\lab-chatbot-with-multi-query-retriever\\temp.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(docs)} documents from {loader.file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Total chunks after splitting: 15\n"
     ]
    }
   ],
   "source": [
    "# show how many total chunks were created after splitting\n",
    "print(f\"📦 Total chunks after splitting: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Import Passages\n",
    "\n",
    "Now that we have split each document into the chunk size of 800, we will now index data to elasticsearch using [ElasticsearchStore.from_documents](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.elasticsearch.ElasticsearchStore.html#langchain.vectorstores.elasticsearch.ElasticsearchStore.from_documents).\n",
    "\n",
    "We will use Cloud ID, Password and Index name values set in the `Create cloud deployment` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This pushes all my chunks (docs) into Elasticsearch to be used for smart search late\n",
    "documents = vectorstore.from_documents(\n",
    "    docs,                                 # the list of text chunks made earlier\n",
    "    embeddings,                            # convert text → numbers (OpenAIEmbeddings)\n",
    "    index_name=\"qa_workplace\",\n",
    "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔮Setting up OpenAI to be our LLM brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)  ## 0 = max accuracy, less creative — perfect for Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 MultiQueryRetriever = Smart Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vectorstore.as_retriever(), # turn our Elastic vectorstore into a retriever\n",
    "    llm                         # to generate multiple query variations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering with MultiQuery Retriever\n",
    "\n",
    "Now that we have the passages stored in Elasticsearch, we can now ask a question to get the relevant passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FINAL BOSS LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough #literally just passes the question along unchanged\n",
    "#get me the docs AND the question at the same time\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "\n",
    "#the multiple search queries GPT creates when you ask a question\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💬 Set Up the Prompt That Talks to GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_CONTEXT_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Be as verbose and educational in your response as possible. \n",
    "    \n",
    "    context: {context}\n",
    "    Question: \"{question}\"\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "LLM_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
    "\n",
    "    #So the AI sees:\n",
    "    \"\"\"  \n",
    "---\n",
    "SOURCE: {name}\n",
    "{page_content}\n",
    "---\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# This function takes the documents and formats them into a string that the LLM can understand.\n",
    "# like 'Here GPT, read this before you answer!'\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=LLM_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "# Build the Chain That Powers Q&A\n",
    "_context = RunnableParallel(\n",
    "    context=retriever | _combine_documents,  # Step 1: Get relevant chunks & format them\n",
    "    question=RunnablePassthrough(),          # Step 2: Just pass the user's question through\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 Hook It Up to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = _context | LLM_CONTEXT_PROMPT | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❓ Finally Ask a Question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the sales team at NASA?', '2. How does the sales team operate within NASA?', '3. What are the responsibilities of the NASA sales team?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Answer ----\n",
      "The NASA sales team is a part of the Americas region in the sales organization of the company. It is led by two Area Vice-Presidents, Laura Martinez for North America and Gary Johnson for South America. The team is responsible for promoting and selling the company's products and services in the North and South American markets. They work closely with other departments, such as marketing, product development, and customer support, to ensure the company's success in these regions.\n"
     ]
    }
   ],
   "source": [
    "ans = chain.invoke(\"what is the nasa sales team?\")\n",
    "\n",
    "print(\"---- Answer ----\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate at least two new iteratioins of the previous cells - Be creative.** Did you master Multi-\n",
    "Query Retriever concepts through this lab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What measures did the company take to assist employees during the pandemic?', '2. In what ways did the company provide support for its employees during the pandemic?', '3. How was the company able to aid its employees during the pandemic?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Answer 1 ----\n",
      "\n",
      "The company supported employees during the pandemic by implementing a full-time work-from-home policy, effective March 2020. This policy provided guidelines and support for employees to conduct their work remotely, ensuring the continuity and productivity of business operations. The policy applied to all eligible employees and allowed them to work from home while maintaining the same level of performance and collaboration as they would in the office. The company also provided necessary equipment and resources for remote work, including a company-issued laptop, software licenses, and access to secure communication tools. Additionally, the company encouraged employees to prioritize their health and well-being while working from home and provided support through regular communication with supervisors, maintaining regular work hours, and taking breaks when needed. The policy was periodically reviewed and updated as necessary to ensure it aligned with public health guidance, business needs, and employee feedback. Employees were also encouraged to direct any questions or concerns about the policy to their supervisor or the HR department.\n"
     ]
    }
   ],
   "source": [
    "ans_1 = chain.invoke(\"How did the company support employees during the pandemic?\")\n",
    "print(\"---- Answer 1 ----\")\n",
    "print(ans_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can I request time off if I am feeling unwell?', '2. Is it possible for me to take a break if I am not feeling well?', '3. What are my options for taking time off if I am not feeling well?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Answer 2 ----\n",
      "\n",
      "Yes, you can take time off if you are not feeling well. According to the Company Vacation Policy, in the event of an unplanned absence due to illness, employees may use their accrued vacation time with supervisor approval. It is important to inform your supervisor as soon as possible and provide any required documentation upon your return to work. Additionally, if your employment is terminated, you will be paid out for any unused vacation time. If you have any further questions or concerns about taking time off, you can direct them to your supervisor or the HR department.\n"
     ]
    }
   ],
   "source": [
    "ans_2 = chain.invoke(\"Can I take time off if I’m not feeling well?\")\n",
    "print(\"---- Answer 2 ----\")\n",
    "print(ans_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA-style Q&A 🛰️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the responsibilities of the Mars mission engineering team?', '2. How does the Mars mission engineering team contribute to the overall mission?', '3. Can you explain the role of the Mars mission engineering team in detail?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Answer 1 ----\n",
      "I'm sorry, I cannot answer this question as it is not mentioned in the provided context. The context only discusses the sales organization and their responsibilities.\n"
     ]
    }
   ],
   "source": [
    "ans_1 = chain.invoke(\"What does the Mars mission engineering team do?\")\n",
    "print(\"---- Answer 1 ----\")\n",
    "print(ans_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the protocols for handling a sick astronaut during a mission?', '2. How does the space agency handle medical emergencies during a space mission?', '3. In the event of an astronaut falling ill during a mission, what steps are taken to ensure their safety and the success of the mission?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Answer ----\n",
      "\n",
      "If an astronaut gets sick on a mission, they will receive medical care from the designated medical officer on board the spacecraft. If the illness is severe or requires specialized treatment, the astronaut may be evacuated to a medical facility on Earth. In some cases, the mission may be altered or shortened to ensure the safety and well-being of the sick astronaut. Additionally, NASA has protocols in place for emergency medical situations, such as a medical emergency on the International Space Station, where the astronaut may be transported back to Earth for treatment.\n"
     ]
    }
   ],
   "source": [
    "ans_2 = chain.invoke(\"What happens if an astronaut gets sick on a mission?\")\n",
    "print(\"---- Answer 2 ----\")\n",
    "print(ans_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
